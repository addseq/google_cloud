{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> GCP POC 1 - Recommendation Engine </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ASSUMPTIONS:</h2>\n",
    "1. Focus is on Collaborative Filtering for now, stay tuned for addition of Content-based recommendation steps.\n",
    "2. Fake data has been created for purpose of this POC that resembles our problem space but with dummy names for Vendors, Products, Services. Data format and column names can be used for final technical implementation.\n",
    "3. Assume that all necessary ETL, Feature Selection, and Feature Engineering has already taken place and input data is in a Model training-ready state.\n",
    "4. Assume Ratings can either be actual user star ratings, or some derived category based on some metric we are capturing, such as # of search hit matches or # of downloads. For e.g. 5 Star  Searched >= 50 times, 4 Star  40 =< Searched < 50, 3 Star  30 =< Searched < 40, etc.\n",
    "\n",
    "<h2> STEPS: </h2>\n",
    "<h3> 1. Import Required PySpark & Misc. Libraries </h3>\n",
    "\n",
    "<b>Note</b>: This Notebook should be run on a GCP Dataproc instance which comes preinstalled w/ PySpark and MLlib libraries for the below imports to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyspark",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3c6ae3c5accd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecommendation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mALS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMatrixFactorizationModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRating\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStructType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFloatType\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pyspark"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from os.path import join, isfile, dirname\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. Configure Parameters for Cloud SQL Connection </h3>\n",
    "\n",
    "Make sure to change the Cloud SQL IP Address and Cloud SQL Password you specified when spinning up the Cloud SQL in GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for Cloud SQL Connection\n",
    "CLOUDSQL_INSTANCE_IP = '104.154.35.194'   # CHANGE (database server IP)\n",
    "CLOUDSQL_DB_NAME = 'recommendation_spark'\n",
    "CLOUDSQL_USER = 'root'\n",
    "CLOUDSQL_PWD = 'cloudsql-pw'  # CHANGE\n",
    "\n",
    "#Optionally pass in the Cloud SQL args\n",
    "#CLOUDSQL_INSTANCE_IP = sys.argv[1]\n",
    "#CLOUDSQL_DB_NAME = sys.argv[2]\n",
    "#CLOUDSQL_USER = sys.argv[3]\n",
    "#CLOUDSQL_PWD  = sys.argv[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3. Configure Parameters for Model Training </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with Parameters- \n",
      "Unique Users: 150\n",
      "Top Predictions Limit: 5\n",
      "ALS Rank: 20\n",
      "ALS Iterations: 20\n",
      "ALS Lambda: 0.01\n",
      "Input Table: Services, Rating_Services\n",
      "Output Table: Recommendation\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Model Training\n",
    "UNIQUE_USER_IDS = 150  # CHANGE\n",
    "LIMIT_TOP_PREDICTIONS = 5  # CHANGE\n",
    "ALS_RANK = 20  # Number of unknown factors that led user to give a rating (E.g. desk, location, age)\n",
    "ALS_ITERATIONS = 20  # Number of times the training will run for various combos of Rank and Lambda\n",
    "ALS_LAMBDA = 0.01  # A regularization parameter to prevent overfitting. Higher value means lower overfitting but greater bias\n",
    "TABLE_SERVICES = 'Services'\n",
    "TABLE_RATINGS = 'Rating_Services'\n",
    "TABLE_RECOMMENDATIONS = 'Recommendation'\n",
    "print(\"Running with Parameters- \"\n",
    "      \"\\nUnique Users: \" + str(UNIQUE_USER_IDS) +\n",
    "      \"\\nTop Predictions Limit: \" + str(LIMIT_TOP_PREDICTIONS) +\n",
    "      \"\\nALS Rank: \" + str(ALS_RANK) +\n",
    "      \"\\nALS Iterations: \" + str(ALS_ITERATIONS) +\n",
    "      \"\\nALS Lambda: \" + str(ALS_LAMBDA) +\n",
    "      \"\\nInput Table: \" + TABLE_SERVICES + \", \" + TABLE_RATINGS +\n",
    "      \"\\nOutput Table: \" + TABLE_RECOMMENDATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4. Initialize SparkSQL Context and JDBC Credentials to connect to Cloud SQL </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkConf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2f8ade250aa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Initialize SparkSQL Context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkConf' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSQL Context\n",
    "conf = SparkConf().setAppName(\"train_model\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Specify JDBC Credentials to connect to Cloud SQL via SparkSQL Context\n",
    "jdbcDriver = 'com.mysql.jdbc.Driver'\n",
    "jdbcUrl = 'jdbc:mysql://%s:3306/%s?user=%s&password=%s' % (CLOUDSQL_INSTANCE_IP, CLOUDSQL_DB_NAME, CLOUDSQL_USER, CLOUDSQL_PWD)\n",
    "\n",
    "# Checkpointing helps prevent stack overflow errors\n",
    "sc.setCheckpointDir('checkpoint/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5. Connect to Cloud SQL and return the stored data as a Dataframe </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the Ratings_Services and Services data from Cloud SQL as DataFrames\n",
    "dfServices = sqlContext.read.format('jdbc').options(driver=jdbcDriver, url=jdbcUrl, dbtable=TABLE_SERVICES).load()\n",
    "dfRatings = sqlContext.read.format('jdbc').options(driver=jdbcDriver, url=jdbcUrl, dbtable=TABLE_RATINGS).load()\n",
    "print(\"Finished Reading: \" + TABLE_SERVICES + \" and \" + TABLE_RATINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 6. Split Dataset randomly to Train (70%) and Test (30%) sets </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the Dataset randomly to Train (70%) and Test (30%) datasets\n",
    "rddTrainData, rddTestData = dfRatings.rdd.randomSplit([7, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 7. Train the Recommendation System model using Alternating Least Squares for Collaborative Filtering method </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the Alternating Least Squares (ALS) Model from Spark MLlib with tunable parameters Rank, Iterations, Lambda\n",
    "# The Rating table should follow the order of service_id, user_id, rating as ALS works with defined product-user pairs\n",
    "model = ALS.train(dfRatings.rdd, ALS_RANK, ALS_ITERATIONS, ALS_LAMBDA)\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 8. Make Predictions with the Trained Model, recommend top 5 Services for each user </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this model to predict what the user would rate Services that he has not rated yet\n",
    "allPredictions = None\n",
    "for USER_ID in xrange(0, UNIQUE_USER_IDS):\n",
    "    # Returns all the Service Ratings given by each User\n",
    "    dfUserRatings = dfRatings.filter(dfRatings.user_id == USER_ID).rdd.map(lambda r: r.service_id).collect()\n",
    "    # Return only Services that have not yet been rated by the User\n",
    "    rddPotential = dfServices.rdd.filter(lambda x: x[0] not in dfUserRatings)\n",
    "    pairsPotential = rddPotential.map(lambda x: (USER_ID, x[0]))\n",
    "    # Calculate all predictions\n",
    "    predictions = model.predictAll(pairsPotential).map(lambda p: (str(p[0]), str(p[1]), float(p[2])))\n",
    "    # Return only top 5 predictions\n",
    "    topPredictions = predictions.takeOrdered(LIMIT_TOP_PREDICTIONS, key=lambda x: -x[2])\n",
    "    print(\"Predicted for User: \" + str(USER_ID), \" Top Predictions: \" + str(topPredictions))\n",
    "    if (allPredictions == None):\n",
    "        allPredictions = topPredictions\n",
    "    else:\n",
    "        allPredictions.extend(topPredictions)\n",
    "print(\"Finished Predicting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 9. Write Predicted results back to Cloud SQL DB in the table Recommendations </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the results of the ML Model from Dataframe back to the Cloud SQL Recommendation table\n",
    "schema = StructType([StructField(\"user_id\", StringType(), True), StructField(\"service_id\", StringType(), True), StructField(\"prediction\", FloatType(), True)])\n",
    "dfToSave = sqlContext.createDataFrame(allPredictions, schema)\n",
    "dfToSave.write.jdbc(url=jdbcUrl, table=TABLE_RECOMMENDATIONS, mode='overwrite')\n",
    "print(\"Finsihed Writing back Predictions to Cloud SQL\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
